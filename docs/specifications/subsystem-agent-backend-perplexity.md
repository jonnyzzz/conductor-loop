# Agent Backend: Perplexity

## Overview
Defines the native Perplexity backend integration for run-agent. Perplexity is treated as an agent type and invoked via REST API (not CLI wrappers).

## Goals
- Provide a stable adapter for Perplexity-backed agent runs.
- Align prompt/input/output handling with other agent backends.

## Non-Goals
- Defining Perplexity account setup or billing.
- Providing advanced vendor-specific features beyond text completion.

## Invocation (REST)
- run-agent sends a prompt to the Perplexity API using an API key from config.hcl (inline or @file reference).
- The adapter writes the response to **stdout** (captured to agent-stdout.txt). `output.md` is generated by the runner from the captured stdout.
- **Streaming Support**: Perplexity API supports streaming via the `stream=True` parameter (Python) or `stream: true` (TypeScript).
  - Uses Server-Sent Events (SSE) format
  - All models support streaming: sonar-pro, sonar-reasoning, sonar-reasoning-pro, sonar-deep-research, r1-1776
  - Response returns chunks that can be iterated over for real-time display
  - Example: `{"model": "sonar", "messages": [...], "stream": True}`
- The adapter should enable streaming and emit tokens to stdout as they arrive to provide real-time progress and avoid idle/stuck detection.
- Citations: Perplexity citations arrive at the end of the stream. The adapter appends a `Sources:` block with numbered URLs (citations or search results).

## I/O Contract
- Input: prompt text (from prompt.md).
- Output: plain text response written to:
  - stdout (captured to agent-stdout.txt) - streams tokens as they arrive from SSE. `output.md` is generated by the runner from this stream.
- Errors: logged to agent-stderr.txt; non-zero exit code for failures.
- Streaming behavior: adapter streams tokens to stdout as they arrive from the API, providing real-time progress visibility.
- Citations: Search results and citations arrive at the end of the stream and are appended to stdout as a `Sources:` list.
- Output file creation: handled by the runner (generic logic) upon successful stream completion.

## Environment / Config
- Tokens/credentials configured in `config.hcl`:
  ```hcl
  agent "perplexity" {
    token = "..."                               # Inline token
    # OR: token = "@/path/to/token.txt"         # @file reference (preferred)
    # OR: token_file = "~/.config/perplexity/token"  # File-based field (alternative)
    # Optional REST settings:
    # api_endpoint = "https://api.perplexity.ai/chat/completions"
    # model = "sonar-reasoning"
  }
  ```
- Runner automatically injects token as `PERPLEXITY_API_KEY` environment variable (hardcoded mapping).
- Token passed to REST adapter (not exposed to agents for workflow use).
- Backend selection uses same round-robin/weights as other agent types.
- Model defaults to most capable (sonar-reasoning) unless overridden in config.
- When config is loaded, run-agent validates agent types and rejects unknown backends.

## Implementation Details (REST/SSE)

### HTTP Request Format
- **URL**: `https://api.perplexity.ai/chat/completions`
- **Method**: POST
- **Headers**:
  - `Authorization: Bearer {PERPLEXITY_API_KEY}`
  - `Content-Type: application/json`
  - `Accept: text/event-stream` (for streaming)
- **Body**:
  ```json
  {
    "model": "sonar-reasoning",
    "messages": [{"role": "user", "content": "..."}],
    "stream": true
  }
  ```

### SSE Stream Parsing
- **Format**: Server-Sent Events with `data:` prefix
- **Events separated by**: Blank lines (`\n\n`); multiple `data:` lines may belong to one event
- **Termination signal**: `data: [DONE]`
- **Content extraction**: `choices[0].delta.content` or `choices[0].message.content` from each JSON chunk
- **Citations**: Appear in final chunks; collect from `citations` (preferred) and `search_results` (fallback)
- **Stream modes**:
  - `stream_mode="full"` (default): All chunks are `chat.completion.chunk`
  - `stream_mode="concise"`: Chunks include `chat.reasoning`, `chat.completion.chunk`, `chat.completion.done`
- **Delta behavior**: Perplexity may send accumulated full text (not just deltas like OpenAI); adapter should diff against the last emitted content

### Error Handling
- **HTTP Status Codes**:
  - 400: Bad Request (invalid parameters) - do not retry
  - 401: Unauthorized (invalid API key) - do not retry
  - 429: Rate Limit Exceeded - retry with backoff
  - 500+: Server Errors - retry with exponential backoff
- **Streaming errors**: Check `finish_reason == "error"` or error objects mid-stream
- **Rate limiting headers**: `x-ratelimit-limit`, `x-ratelimit-remaining`, `x-ratelimit-reset`
- **Retry strategy**: Use `retry-after` header or exponential backoff (1s to 32s) with jitter; max 5 attempts

### Timeout Configuration
- **Connect timeout**: 10 seconds
- **TLS handshake**: 10 seconds
- **Response header timeout**: 10 seconds (time to first byte)
- **Idle timeout (streaming)**: 60 seconds (models can pause during "thinking")
- **Total request timeout**: ~2 minutes (complex queries take time)

### Go Implementation Notes
- Use `bufio.Scanner` for line-based SSE parsing
- Strip `data:` prefix and check for `[DONE]` marker
- Handle both delta and accumulated content modes
- Collect search_results/citations from final chunks and append a `Sources:` list
- Implement retry loop for 429/5xx errors with exponential backoff

## Related Files
- subsystem-runner-orchestration.md
- subsystem-env-contract.md
- PERPLEXITY-API-HTTP-FORMAT.md (detailed implementation guide)
