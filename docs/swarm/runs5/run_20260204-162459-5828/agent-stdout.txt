I will read the necessary files to review "Topic 8: Agent Backend Integrations" and check the git history for context.
I will check the git status and re-read the question files to ensure I have the correct content.
Here is the review of Topic 8: Agent Backend Integrations.

### 1) External facts check

*   **Claude CLI:** `ANTHROPIC_API_KEY` is the standard environment variable for the TypeScript-based `claude` CLI (and most SDKs).
*   **Gemini CLI:** The standalone CLI (often `google-generativeai` or community tools) typically uses `GOOGLE_API_KEY` or `GEMINI_API_KEY`. The `gemini` command availability depends on the specific installed package; assuming a standard wrapper. Streaming is generally supported by the underlying API.
*   **Perplexity:** API is `https://api.perplexity.ai`. Current "reasoning" models are `sonar-reasoning-pro` (and `sonar-reasoning`). `sonar-pro` is the standard "smart" model.
*   **xAI:** API is compatible with OpenAI SDKs. Models include `grok-2` and `grok-beta`. OpenCode integration would likely use an OpenAI-compatible endpoint configuration.

### 2) TIPICS updates

*   **Topic 8: Decisions**
    *   Clarify: "Claude, Codex, and Gemini are invoked via CLI wrappers."
    *   Clarify: "Perplexity is a native REST-backed integration (no CLI wrapper)."
    *   Clarify: "xAI is deferred; likely to use OpenCode with OpenAI-compatible endpoint configuration."
    *   Add: "Backends must support unbuffered/streaming output where possible to satisfy liveness checks."
*   **Topic 8: Open questions**
    *   Update: "Specific environment variable mapping for Gemini and xAI."
    *   Update: "Handling of citations/references for Perplexity (append vs. structured)."
    *   Remove: "Backend-specific env var mappings" (if resolved for Claude/Gemini).

### 3) Spec updates

*   **subsystem-agent-backend-gemini.md**
    *   Update `Environment / Config`: Explicitly state `GEMINI_API_KEY` as the injected variable.
    *   Update `Invocation`: Add note to ensure stdout is unbuffered (e.g. `PYTHONUNBUFFERED=1` if it were python, or CLI specific flags).

*   **subsystem-agent-backend-perplexity.md**
    *   Update `Environment / Config`: Set default model to `sonar-reasoning-pro` (or latest reasoning variant) for "smart" tasks.
    *   Update `I/O Contract`: Specify that citations should be appended to `output.md` as a "References" section.

*   **subsystem-agent-backend-xai.md**
    *   Update `Invocation`: Note that xAI offers an OpenAI-compatible API, so the "OpenCode" integration will likely just be a configuration of the generic OpenAI/Codex adapter with a custom base URL.

### 4) Questions to remove/resolve

*   **subsystem-agent-backend-claude-QUESTIONS.md**
    *   *Q: What environment variable name does the Claude CLI expect...?*
    *   **Resolution:** `ANTHROPIC_API_KEY`. (Already answered in file, can be archived/removed).

*   **subsystem-agent-backend-gemini-QUESTIONS.md**
    *   *Q: What environment variable name does the Gemini CLI expect...?*
    *   **Resolution:** `GEMINI_API_KEY` (to avoid conflict with generic `GOOGLE_API_KEY` used by gcloud, though SDKs accept both. Prefer `GEMINI_API_KEY` for run-agent config -> env mapping).
    *   *Q: Does the Gemini CLI support streaming/unbuffered stdout...?*
    *   **Resolution:** Yes. The runner should assume it does and enforce unbuffered execution.

*   **subsystem-agent-backend-perplexity-QUESTIONS.md**
    *   *Q: Which Perplexity model should be the default...?*
    *   **Resolution:** `sonar-reasoning-pro` for coding/logic tasks.
    *   *Q: What timeout and retry policy...?*
    *   **Resolution:** Standard runner backoff (1s, 2s, 4s). Request timeout 120s (reasoning models can be slow).
    *   *Q: How should Perplexity citations be represented...?*
    *   **Resolution:** Append to the end of `output.md`.

*   **subsystem-agent-backend-xai-QUESTIONS.md**
    *   *Q: Which xAI model/agent should be targeted first...?*
    *   **Resolution:** `grok-2` (or latest). Use OpenAI-compatible REST API.
    *   *Q: Does the xAI backend require additional tool-calling...?*
    *   **Resolution:** No, start with text completion.

### 5) New open questions

*   **subsystem-agent-backend-xai-QUESTIONS.md**
    *   Q: Should xAI be implemented as a distinct "agent backend" or just a configuration profile for the `codex` (OpenAI-compatible) backend?
    *   Proposed default: Configuration profile for `codex` to reduce code duplication.
    *   A: TBD.

*   **subsystem-agent-backend-perplexity-QUESTIONS.md**
    *   Q: Does Perplexity API support streaming (SSE) for the reasoning process, and should run-agent consume it?
    *   Proposed default: Yes, consume SSE to update `output.md` in real-time and prevent idle timeouts.
    *   A: TBD.
