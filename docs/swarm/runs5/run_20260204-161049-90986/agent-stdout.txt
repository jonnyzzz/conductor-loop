Perfect! Now I have enough information. Let me analyze the documents and provide my review:

---

## Review of Agent Backend: Perplexity Design

### 1) External Facts Check

**Verified via web search:**

✅ **Streaming Support**: Perplexity API supports streaming via SSE (Server-Sent Events). Set `stream=True` (Python) or `stream: true` (TypeScript). Official docs at https://docs.perplexity.ai/guides/streaming-responses

✅ **Citations Format**: Perplexity returns citations in a `citations` array (URLs) with inline markers like `[1][2]` in the content. As of Nov 2024, citations are publicly available and included by default in chat completions.

⚠️ **Streaming Behavior Note**: Perplexity does NOT use OpenAI's `chat.completion.chunk` spec. Streamed content contains the complete content up to that point, NOT incremental deltas.

✅ **Models**: Perplexity offers Sonar, Sonar Pro, and reasoning-capable models (sonar-reasoning mentioned in questions aligns with available models).

**No contradictions found with external facts.**

---

### 2) Spec Updates

**File: `subsystem-agent-backend-perplexity.md`**

- Add after line 17 (Invocation section):
  - "Adapter should emit periodic progress (or stream tokens if supported) to avoid idle/stuck detection."
  
- Add new section after "Invocation (REST)":
  ```markdown
  ## Streaming & Progress
  - Perplexity API supports streaming via SSE (set stream=true).
  - Adapter should use streaming mode to provide liveness signals to the runner's idle/stuck detector.
  - Note: Perplexity streams complete content (not incremental deltas); parse SSE format accordingly.
  ```

- Add new section after "I/O Contract":
  ```markdown
  ## Citations & References
  - Perplexity responses include a citations array (URLs) with inline citation markers ([1][2]) in content.
  - Adapter should append citations as a "References" section in output.md after the main response.
  - Format: numbered list matching inline citation markers.
  ```

- Update "Environment / Config" section (line 24-27):
  - Add: "Config includes a Perplexity section; use the most capable model by default unless overridden."

**File: `TIPICS.md`**

- Topic 8 (Agent Backend Integrations), line 107:
  - Add bullet: "Perplexity backend uses REST with SSE streaming; citations appended to output.md as References section."

---

### 3) Questions to Remove/Resolve

**File: `subsystem-agent-backend-perplexity-QUESTIONS.md`**

**Q1 (lines 3-5): Model selection**
- **Resolution**: ANSWERED in git history (commit 43e152d).
- **Answer**: "Yes, introduce perplexity section in the config, use the most smart model by default."
- **Action**: REMOVE this question; decision is incorporated into spec line 27.

**Q2 (lines 7-9): Timeout and retry policy**
- **Resolution**: ANSWERED in git history (commit 43e152d).
- **Answer**: "We tend to run the tool with progress updates, we measure if the tool is not quiet for some time, the same as for other agent tools"
- **Action**: REMOVE this question; aligns with runner's idle/stuck detection (no Perplexity-specific timeout needed).

**Q3 (lines 3-4 in QUESTIONS.md current state): Streaming for liveness**
- **Resolution**: ANSWERED by external facts check + git history context.
- **Answer**: Yes, use streaming (SSE) for liveness/progress. Perplexity supports SSE streaming.
- **Action**: REMOVE this question after incorporating into spec (see section 2 above).

**Q4 (lines 4-5 in QUESTIONS.md current state): Citations representation**
- **Resolution**: ANSWERED by external facts check.
- **Answer**: Append "References" section with numbered list matching inline citations.
- **Action**: REMOVE this question after incorporating into spec (see section 2 above).

---

### 4) New Open Questions

**File: `subsystem-agent-backend-perplexity-QUESTIONS.md`**

**New Q1**: Should the Perplexity adapter use the Chat Completions API or the newer Agentic Research API for run-agent tasks?  
**Proposed default**: Use Chat Completions API in MVP; Agentic Research API is intended for multi-step research workflows.  
**A**: TBD.

**New Q2**: Should citation URLs be validated or sanitized before writing to output.md?  
**Proposed default**: No validation in MVP; write citations as-is from API response.  
**A**: TBD.

**New Q3**: How should the adapter handle Perplexity-specific errors (rate limits, quota exhausted, invalid API key)?  
**Proposed default**: Auth/quota errors fail fast (non-zero exit, message to stderr); rate limits use transient backoff (1s/2s/4s, max 3 tries) per runner policy.  
**A**: TBD.

**New Q4**: Should the adapter expose Perplexity-specific parameters (e.g., search_domain_filter, return_related_questions) via config or per-task overrides?  
**Proposed default**: Not in MVP; use API defaults. Consider post-MVP if use cases emerge.  
**A**: TBD.

---

### 5) New Docs/Topics Suggested

**No new dedicated docs needed.** The Perplexity backend spec is appropriately scoped and aligns with the existing subsystem structure. All integration details fit within the current `subsystem-agent-backend-perplexity.md` spec.

**Minor note**: Consider adding a cross-reference in `subsystem-runner-orchestration.md` (section on backend adapters) to explicitly list Perplexity as a native REST-backed agent type alongside Claude/Gemini, differentiating it from CLI-wrapped backends like Codex.

---

**Sources:**
- [Streaming Responses - Perplexity](https://docs.perplexity.ai/guides/streaming-responses)
- [Perplexity API Citations](https://blog.hypertxt.ai/2024/11/08/perplexity-api-citations/)
- [Enhancement: Perplexity Completion response incl. citations](https://github.com/danny-avila/LibreChat/discussions/4692)
- [Feature: Fix the incorrect streaming response of Perplexity API](https://github.com/BerriAI/litellm/issues/8455)
