Perplexity MCP is not configured correctly. Let me proceed with the review based on the specifications and git history.

Now let me compile my review findings:

---

## 1) External Facts Check

**Unable to verify external claims** due to Perplexity MCP configuration issues (401 Unauthorized). The following external factual claims in the specs should be verified during implementation:

- **Claude CLI flags**: `--permission-mode bypassPermissions`, `--tools default`, `--input-format text`, `--output-format text`
- **Codex CLI flags**: `--dangerously-bypass-approvals-and-sandbox`, `-C <cwd>`
- **Gemini CLI flags**: `--screen-reader true`, `--approval-mode yolo`
- **Perplexity API**: Model names (sonar-pro, sonar-reasoning), streaming support
- **OpenCode + xAI**: Whether OpenCode supports xAI models via configuration
- **HCL format**: Confirmed as Hashicorp Configuration Language (widely known, low risk)
- **SSE vs WebSocket**: Both are valid streaming protocols (low risk claim)

**Recommendation**: These CLI flags and API details should be validated against actual tool documentation during implementation.

---

## 2) TIPICS.md Updates

**Topic 5: Configuration & Backend Selection**

Current state is mostly complete based on git history (commit 43e152d resolved many questions). Proposed updates:

- **Add decision**: "Model selection delegates to host CLI defaults; run-agent does not override model names unless explicitly configured in config.hcl."
- **Add decision**: "All agent backends run with full tool access and no sandboxing (bypass approvals/permissions flags are standard)."
- **Update decision**: Change "xAI integration is deferred post-MVP" to explicitly state "xAI integration via OpenCode is deferred post-MVP; requires xAI token in config to enable."
- **Update decision**: Clarify config token format: "Tokens may be specified inline as strings or via @/path/to/file references."
- **Add decision**: "Backend-specific environment variable mapping (e.g., ANTHROPIC_API_KEY, GEMINI_API_KEY) is handled by run-agent based on config.hcl sections."
- **Update open questions**: Remove "xAI integration strategy" (answered: deferred post-MVP). Remove "Dedicated design documents per agent type" (already exists: subsystem-agent-backend-*.md files).
- **Add open question**: "Should config.hcl support per-backend timeout/retry overrides beyond global defaults?"
- **Add open question**: "How should config schema evolution/versioning be handled for backwards compatibility?"

---

## 3) Spec Updates

### subsystem-runner-orchestration.md

- **Line 70**: Add clarification: "Token format: inline string or @/path/to/file reference (@ prefix indicates file path)."
- **Line 82**: Add detail: "Selection may consult message bus history to avoid repeatedly failing backends within a time window."
- **Line 85**: Clarify: "xAI integration via OpenCode is deferred post-MVP (see subsystem-agent-backend-xai.md)."
- **Section "Agent Backends"**: Add note: "Each backend adapter is responsible for injecting credentials via the appropriate environment variable name (e.g., ANTHROPIC_API_KEY for Claude, GEMINI_API_KEY for Gemini)."
- **Section "Error Handling"**: Add: "Proactive credential validation is not performed; auth failures are detected at agent spawn time."
- **Section "Configuration"**: Add bullet: "config.hcl schema is embedded in the Go binary; validation occurs on run-agent startup."

### subsystem-agent-backend-codex.md

- **Line 28**: Clarify: "No sandboxing enforced; full tool access is required (keep `--dangerously-bypass-approvals-and-sandbox`)."
- **Line 29**: Add: "run-agent does not set model/reasoning flags; agent relies on host CLI config and defaults."
- **Add new section "Credentials"**: "Codex CLI credentials are assumed to be configured via host system (e.g., ~/.config/codex or similar). run-agent does not inject tokens for Codex."

### subsystem-agent-backend-claude.md

- **Line 28**: Add: "Credentials injected via ANTHROPIC_API_KEY environment variable (mapped from config.hcl)."
- **Line 29**: Clarify: "run-agent does not set a model; host CLI defaults/config are used unless overridden in config.hcl."
- **Add new section "Environment Variables"**: "ANTHROPIC_API_KEY is injected by run-agent from config.hcl (inline or @file reference)."

### subsystem-agent-backend-gemini.md

- **Line 28**: Add: "Credentials injected via GEMINI_API_KEY environment variable (mapped from config.hcl)."
- **Line 29**: Clarify: "run-agent does not set a model; host CLI defaults/config are used unless overridden in config.hcl."
- **Add new section "Environment Variables"**: "GEMINI_API_KEY is injected by run-agent from config.hcl (inline or @file reference)."

### subsystem-agent-backend-perplexity.md

- **Line 16**: Clarify: "Adapter implemented as a native Go client within run-agent (not a separate CLI tool)."
- **Line 27**: Add detail: "Use the most capable reasoning model by default (e.g., sonar-reasoning-pro if available)."
- **Add new section "Environment Variables"**: "PERPLEXITY_API_KEY is read from config.hcl by run-agent (inline or @file reference); not exposed to agents as an environment variable."
- **Add new section "Streaming"**: "Perplexity adapter should emit periodic progress logs or stream tokens (if API supports it) to avoid idle/stuck detection triggers."

### subsystem-agent-backend-xai.md

- **Line 14**: Clarify: "Use OpenCode CLI configured to target xAI models (only if xAI token is provided in config.hcl)."
- **Line 15**: Add: "Default to the best available xAI model (e.g., Grok) when enabled post-MVP."
- **Line 23**: Add: "API key stored in config.hcl as XAI_API_KEY (inline or @file reference); passed to OpenCode via appropriate flags or environment variables."
- **Add note**: "This integration is deferred post-MVP. See ideas.md for rationale."

### subsystem-message-bus-tools.md

No changes needed; this subsystem is orthogonal to backend selection.

---

## 4) Questions to Remove/Resolve

### subsystem-runner-orchestration-QUESTIONS.md

- **Q: "What is the xAI integration strategy..."**
  - **Resolution**: Answered in git commit 43e152d: "We are not going to support xAI just now, park the work for post-MVP." xAI integration via OpenCode is deferred. This question can be **removed** and the decision is already in TIPICS.md.

### subsystem-agent-backend-claude-QUESTIONS.md

- **Q: "Which Claude model should be the default..."**
  - **Resolution**: Answered in git commit 43e152d: "Right now we do not specify a model to Claude Code at all. Let's assume it's configured on the host system the correct way." This question can be **removed** with a note in the spec that model selection is delegated to host CLI config.

- **Q: "Should tool access be restricted..."**
  - **Resolution**: Answered in git commit 43e152d: "All tools should be accessible and allowed. Review how ../run-agent.sh is done, it has to be the same for all tasks." This question can be **removed**.

### subsystem-agent-backend-codex-QUESTIONS.md

- **Q: "Should run-agent allow overriding the Codex model..."**
  - **Resolution**: Answered in git commit 43e152d: "See Claude answers" (i.e., use host CLI defaults). This question can be **removed**.

- **Q: "Should the Codex adapter always run with --dangerously-bypass-approvals-and-sandbox..."**
  - **Resolution**: Answered in git commit 43e152d: "We start all agents with `--dangerously-bypass-approvals-and-sandbox` for now, make sure we pass agent-specific flags for all agents." This question can be **removed**.

### subsystem-agent-backend-gemini-QUESTIONS.md

- **Q: "Should --approval-mode yolo remain the default..."**
  - **Resolution**: Answered in git commit 43e152d: "We start all agents with full permissions and without sandbox." This question can be **removed**.

- **Q: "Which Gemini model should be the default..."**
  - **Resolution**: Answered in git commit 43e152d: "See the answer for Claude." (i.e., use host CLI defaults). This question can be **removed**.

- **Q: "What environment variable name does the Gemini CLI expect..."**
  - **Resolution**: Partially answered; GEMINI_API_KEY is the proposed default. This question can be **removed** with the assumption that GEMINI_API_KEY is standard (should be verified during implementation).

- **Q: "Does the Gemini CLI support streaming..."**
  - **Resolution**: Not explicitly answered. **Keep** this question but rephrase: "Does the Gemini CLI support unbuffered stdout for live progress updates, and if so, are additional flags required?"

### subsystem-agent-backend-perplexity-QUESTIONS.md

- **Q: "Which Perplexity model should be the default..."**
  - **Resolution**: Answered in git commit 43e152d: "Yes, introduce perplexity section in the config, use the most smart model by default." This question can be **removed**.

- **Q: "What timeout and retry policy..."**
  - **Resolution**: Answered in git commit 43e152d: "We tend to run the tool with progress updates, we measure if the tool is not quiet for some time, the same as for other agent tools." This question can be **removed**.

- **Q: "Should the Perplexity adapter use streaming..."**
  - **Resolution**: Partially answered (progress updates expected). **Rephrase and keep**: "Does the Perplexity API support token streaming for responses, and should the adapter emit periodic progress logs if not?"

- **Q: "How should Perplexity citations be represented..."**
  - **Resolution**: Not answered. **Keep** this question but add context: "Perplexity responses may include citations/references; should these be appended as a 'References' section in output.md or left inline?"

### subsystem-agent-backend-xai-QUESTIONS.md

- **Q: "Which xAI model/agent should be targeted..."**
  - **Resolution**: Answered in git commit 43e152d: "We use OpenCode agent, which needs to be configured to use xAI. Only if the token is provided (same for all agents), use the best model by default... Park this to TODOs, we address the feature later after the MVP." This question can be **removed** (work deferred).

- **Q: "Does the xAI backend require additional tool-calling..."**
  - **Resolution**: Answered in git commit 43e152d: "We run all tools without sandboxing or restrictions." This question can be **removed**.

- **Q: "How will OpenCode be configured to use xAI models..."**
  - **Resolution**: Partially answered (use OpenCode + xAI token). **Keep** this question since the integration is deferred: "How will OpenCode be configured to use xAI models (flags, env vars, config file) when post-MVP integration begins?"

### subsystem-agent-backend-codex-QUESTIONS.md (current state)

- **File states "No open questions at this time."**
  - **Add new question**: "What environment variable or config mechanism does Codex CLI use for API credentials, and should run-agent inject it?"

---

## 5) New Open Questions

### subsystem-runner-orchestration-QUESTIONS.md

- **Q**: "Should config.hcl support per-backend timeout/retry overrides beyond the global exponential backoff (1s, 2s, 4s)?" | Proposed default: No; use global defaults for MVP. | A: TBD.

- **Q**: "How should config.hcl schema versioning be handled for backwards compatibility as the format evolves?" | Proposed default: Embed a schema_version field; validate on load and reject incompatible versions. | A: TBD.

### subsystem-agent-backend-claude-QUESTIONS.md

- **Q**: "What environment variable name does the Claude CLI expect for API credentials (e.g., ANTHROPIC_API_KEY), and how should run-agent map config keys to it?" | Proposed default: Map ANTHROPIC_API_KEY from config.hcl. | A: TBD.

### subsystem-agent-backend-gemini-QUESTIONS.md

- **Q**: "What environment variable name does the Gemini CLI expect for API credentials?" | Proposed default: GEMINI_API_KEY. | A: TBD.

- **Q**: "Does the Gemini CLI support streaming/unbuffered stdout so the UI can show live progress?" | Proposed default: Yes; enforce unbuffered output if supported. | A: TBD.

### subsystem-agent-backend-perplexity-QUESTIONS.md

- **Q**: "Should the Perplexity adapter use streaming responses for liveness/progress updates?" | Proposed default: Yes, if supported; otherwise emit periodic progress logs. | A: TBD.

- **Q**: "How should Perplexity citations be represented in output.md?" | Proposed default: Append a "References" section after the main response. | A: TBD.

### subsystem-agent-backend-xai-QUESTIONS.md

- **Q**: "How will OpenCode be configured to use xAI models when the integration is enabled post-MVP?" | Proposed default: Provide a run-agent adapter that passes model/provider flags to OpenCode based on config.hcl. | A: TBD.

### subsystem-agent-backend-codex-QUESTIONS.md

- **Q**: "What environment variable or config mechanism does Codex CLI use for API credentials, and should run-agent inject it from config.hcl?" | Proposed default: Assume Codex uses host system config (~/.config/codex or similar); no injection needed. | A: TBD.

---

## Summary

**External facts check**: Unable to verify due to Perplexity MCP auth failure; CLI flags and API names should be validated during implementation.

**TIPICS updates**: Add decisions on model selection defaults, full tool access policy, token format, env var mapping, and clarify xAI deferral. Remove resolved open questions; add new questions on config schema versioning and per-backend overrides.

**Spec updates**: Clarify token injection mechanisms, environment variable names, credential handling, and streaming/progress requirements across all backend specs. Add missing sections for environment variables and credentials.

**Questions to remove**: 11 questions resolved via git commit 43e152d (xAI deferral, model selection, full tool access, Perplexity config).

**New open questions**: 8 new questions covering environment variable names, streaming support, config schema versioning, and OpenCode+xAI integration details.
